---
title: hw4_110081014

---

# Implementation (seq-flashattention.c ) 
[hw4_flash.cu](https://github.com/shirou1217/PP24/blob/main/hw4/hw4_flash.cu)
### Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (â„“ and ğ‘š) were calculated.
- å¤š Batch ä¸€æ¬¡æ€§è™•ç†
    - ä¸€æ¬¡æ€§æŠŠæ‰€æœ‰ BNd çš„ Q, K, V, O æ‹·è²åˆ° GPU (ä»¥åŠç‚º batch l[]ã€m[] åˆ†é…ç©ºé–“)ï¼Œ
    - GPU è£é¢ç”¨ä¸€å€‹å‡½å¼ (flash_attention_all_batches) æŠŠæ¯å€‹ batch çš„æ³¨æ„åŠ›é‹ç®—å…¨éƒ¨åšå®Œ
    - æœ€å¾Œå†æŠŠğ‘‚ä¸€æ¬¡æ€§æ‹·å› Hostã€‚
```shell=
    // 3) ä¸€æ¬¡æ€§æ‹· Q, K, V, O
    cudaMemcpy(d_Q, Q, B*N*d*sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_K, K, B*N*d*sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_V, V, B*N*d*sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_O, O, B*N*d*sizeof(float), cudaMemcpyHostToDevice);
    // 4) åŸ·è¡Œ
    flash_attention_all_batches(br, bc);
    // 5) æŠŠ O æ‹·å› Host
    cudaMemcpy(O, d_O, B*N*d*sizeof(float), cudaMemcpyDeviceToHost);
    // 6) è¼¸å‡º
    output(argv[2]);
```
- SRAM usage & matrix blocking
    - QKDotAndScalarKernel ä¸­ä½¿ç”¨tiled matrix multiplicationæŠ€å·§å°dåšåˆ†æ®µï¼ˆtilePos += 16ï¼‰ï¼Œåœ¨ shared memory ä¸­ä¸€æ¬¡è¼‰å…¥ 16 å€‹ç‰¹å¾µç¶­åº¦ï¼Œç„¶å¾Œ partial dot product
```shell=
+---------------------------+
|        QKDotAndScalarKernel        |
| Grid: (number of iBlock, jBlock)  |
| Block: (TILE_SIZE, TILE_SIZE)      |
+---------------------------+
            |
            v
+-------------------------------+
| æ¯å€‹ Thread è² è²¬ä¸€å€‹ (i, j)   |
+-------------------------------+
            |
            v
+---------------------------------------+
| 1. è¼‰å…¥ Q èˆ‡ K çš„ tile åˆ° Shared Memory |
|    - sQ[TILE_SIZE][TILE_SIZE]          |
|    - sK[TILE_SIZE][TILE_SIZE]          |
+---------------------------------------+
            |
            v
+-------------------------------+
| 2. è¨ˆç®—éƒ¨åˆ† Dot Product       |
|    - for(t = 0; t < TILE_SIZE; t++) |
|      sum += sQ[tidx][t] * sK[tidy][t] |
+-------------------------------+
            |
            v
+-------------------------------+
| 3. ç¸½å’Œèˆ‡ä¹˜ä¸Š scalar         |
|    - out[i * bc + j] = sum * scalar  |
+-------------------------------+

```

- â„“ and ğ‘š è¨ˆç®—
- åˆå§‹åŒ– â„“ å’Œ ğ‘š
    - initLMForBatchã€€kernelåœ¨æ¯å€‹ batch çš„æ¯ä¸€è¡Œé–‹å§‹è¨ˆç®—å‰ï¼Œå°‡ â„“ åˆå§‹åŒ–ç‚º 0ï¼Œğ‘š åˆå§‹åŒ–ç‚º -âˆ
```shell=
__global__ void initLMForBatch(float *d_l, float *d_m, int bIdx, int N)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x; // [0..N)
    if(i < N){
        int offset = bIdx*N + i;
        d_l[offset] = 0.0f;     // l=0
        d_m[offset] = -FLT_MAX; // m = -âˆ
    }
}
```
- åˆ†å¡Šè¨ˆç®—èˆ‡ â„“ã€ğ‘š çš„æ›´æ–°
    - æ•´å€‹ QK^TçŸ©é™£è¢«åˆ†å‰²æˆè‹¥å¹²å€‹å°å€å¡Šï¼ˆtilesï¼‰ï¼Œæ¯å€‹å€å¡Šçš„å¤§å°ç‚º brÃ—bcï¼ˆä¾‹å¦‚32Ã—32ï¼‰ã€‚å°æ–¼æ¯å€‹å€å¡Šï¼ŒåŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š
    1. è¨ˆç®—å€å¡Šå…§çš„æ³¨æ„åŠ›å¾—åˆ† Sij `QKDotAndScalarKernel<<<grid, block>>>(d_sij, d_Q, d_K, bIdx, iBlock, jBlock, br, bc, N, d, scale);`
    2. æ‰¾å‡ºå€å¡Šå…§æ¯ä¸€è¡Œçš„æœ€å¤§å€¼ mij `RowMaxKernel<<<br,1>>>(d_mij, d_sij, br, bc);`
    3. è¨ˆç®—exp(Sijâˆ’mi)`MinusMaxAndExpKernel<<<grid, block>>>(
    d_pij, d_sij, d_mij, br, bc);`
    4. è¨ˆç®—å€å¡Šå…§æ¯ä¸€è¡Œçš„ç´¯åŠ å’Œlij`RowSumKernel<<<br,1>>>(d_lij, d_pij, br, bc);`
    5. æ›´æ–°å…¨å±€çš„ â„“ å’Œ ğ‘šä¸¦æ›´æ–°è¼¸å‡ºğ‘‚`UpdateMiLiOiKernel<<<grid, block>>>(d_m, d_l, d_O, d_pij, d_mij, d_lij, d_V, bIdx, iBlock, jBlock, br, bc, N, d);`

### Explain how matrices Q, K, and V are divided into blocks and processed in parallel.
1. divided into blocksï¼š
å°‡ Q å’Œ K åŠƒåˆ†ç‚º br Ã— bc çš„å¡Šã€‚
æ¯å€‹ CUDA Block è™•ç†ä¸€å€‹ Q å’Œ K çš„å¡Šå°ï¼ˆiBlock, jBlockï¼‰ã€‚
2. load into Shared Memoryï¼š
æ¯å€‹ Thread è² è²¬è¼‰å…¥ TILE_SIZE Ã— TILE_SIZE çš„å­å¡Š sQ å’Œ sK åˆ°å…±äº«è¨˜æ†¶é«”ï¼Œä»¥å¯¦ç¾é«˜æ•ˆçš„é»ç©è¨ˆç®—ã€‚sQ å’Œ sK æ˜¯å…±äº«è¨˜æ†¶é«”ä¸­çš„æš«å­˜å€ï¼Œç”¨æ–¼å­˜æ”¾ç•¶å‰å¡Šçš„ Q å’Œ K çš„å­å¡Šã€‚
é»ç©è¨ˆç®—ï¼šæ¯å€‹ Thread åœ¨å…±äº«è¨˜æ†¶é«”ä¸­è¨ˆç®—éƒ¨åˆ†é»ç©ï¼Œç„¶å¾Œç´¯åŠ åˆ°ç¸½å’Œ sum ä¸­ã€‚ç¶“éå¤šæ¬¡è¼‰å…¥å­å¡Š sQ å’Œ sKï¼Œå®Œæˆæ•´å€‹é»ç©çš„è¨ˆç®—ã€‚
3. å¯«å›çµæœï¼šå°‡è¨ˆç®—çµæœ sum ä¹˜ä»¥ç¸®æ”¾å› å­ scalarï¼Œç„¶å¾Œå¯«å›å…¨åŸŸè¨˜æ†¶é«”ä¸­çš„è¼¸å‡ºçŸ©é™£ d_sijã€‚
```shell=
+-----------------------------------+
|         åŸå§‹çŸ©é™£ Q, K, V            |
|         (B Ã— N Ã— d)                |
+-----------------------------------+
                |
                v
    +-------------------------------+
    |     åˆ†å‰²æˆ br Ã— bc çš„å°å¡Š       |
    |     (ä¾‹å¦‚ br = 32, bc = 32)    |
    +-------------------------------+
                |
                v
    +-------------------------------+
    |  ä¸¦è¡Œè™•ç†æ¯å€‹ (iBlock, jBlock) |
    +-------------------------------+
                |
                v
    +-------------------------------+       +-----------------------------+
    | QKDotAndScalarKernel          | ----> | RowMaxKernel                |
    | - Load Q and K tiles into     |       | - Find row-wise maximum in   |
    |   shared memory               |       |   S_ij                      |
    | - Compute partial dot product |       | - Store in d_mij             |
    | - Store S_ij in d_sij         |       +-----------------------------+
    +-------------------------------+                   |
                |                                       v
                v                           +-----------------------------+
    +-------------------------------+       | MinusMaxAndExpKernel        |
    | RowMaxKernel                  | ----> | - Compute exp(S_ij - m_i)   |
    | - Find row-wise maximum in     |       | - Store in d_pij             |
    |   S_ij                        |       +-----------------------------+
    | - Store in d_mij               |                   |
    +-------------------------------+                   v
                |                           +-----------------------------+
                v                           | RowSumKernel                |
    +-------------------------------+       | - Compute row-wise sum of    |
    | MinusMaxAndExpKernel          | ----> |   p_ij                      |
    | - Compute exp(S_ij - m_i)     |       | - Store in d_lij             |
    | - Store in d_pij               |       +-----------------------------+
    +-------------------------------+                   |
                |                                       v
                v                           +-----------------------------+
    +-------------------------------+       | UpdateMiLiOiKernel          |
    | RowSumKernel                  | ----> | - Update l and m            |
    | - Compute row-wise sum of      |       | - Update output O            |
    |   p_ij                        |       +-----------------------------+
    | - Store in d_lij               |
    +-------------------------------+
                |
                v
    +-------------------------------+
    |  æ›´æ–°è¼¸å‡ºçŸ©é™£ O                 |
    +-------------------------------+
                |
                v
    +-------------------------------+
    |    ä¸‹ä¸€å€‹ Tile çš„è™•ç†           |
    +-------------------------------+

```

### Describe how you chose the block sizes B_râ€‹ and B_câ€‹ and why.
æˆ‘é¸æ“‡ block size ğµğ‘Ÿ=32 Bc=32ä»¥ä¸‹æ˜¯æˆ‘çš„ç†ç”±
- æ¸›å°‘ Global Memory è®€å¯«æ¬¡æ•¸ã€æå‡å¹³è¡Œåº¦
    - ä¸€æ¬¡è™•ç† 32 æ¢ row (æˆ– column) å¯ä»¥è®“ GPU kernel åœ¨ä¸€å€‹å€å¡Š (iBlock, jBlock) è£¡åšè¶³å¤ å¤šçš„é‹ç®—ï¼Œæ¸›å°‘ kernel å‘¼å«çš„ overheadã€‚
    - åŒæ™‚ 32 é€šå¸¸èˆ‡ warp size (32 threads) æœ‰å°é½Šçš„å„ªå‹¢ï¼Œå° row-based kernel æœ‰åŠ©æ–¼ coalescingã€‚
- é…åˆå¾ŒçºŒ Shared Memory Tiling (TILE_SIZE=16)ï¼š
    - åœ¨ QKDotAndScalarKernel è£é¢ï¼Œæˆ‘å€‘åˆé€²ä¸€æ­¥å°‡æ¯æ¬¡è¦ç”¨çš„ d-dimension(ç‰¹å¾µç¶­åº¦)åˆ†æˆ tiles å¤§å° = 16ã€‚
    - é€™æ¨£ 32 x 32 çš„ S å€å¡Šï¼Œåœ¨ kernel å…§éƒ¨å°±æœƒç”¨ 16 x 16 çš„ tile å…©æ¬¡ (å› ç‚º 32 Ã· 16 = 2) æƒé row / columnã€‚
- è€ƒé‡ Shared Memory å®¹é‡ï¼š
    - QKDotAndScalarKernel åœ¨æ¯æ¬¡ 16 ç¶­ç‰¹å¾µè¦è¼‰å…¥ shared memory çš„é‡ç‚º 16Ã—16
16Ã—16ï¼ˆå° Q åŠ K å„éœ€è¦ä¸€å¡Šï¼‰ï¼Œå…± 512 å€‹ floatï¼ˆç´„ 2KBï¼‰ï¼Œå°å¤§å¤šæ•¸ GPU è€Œè¨€ç›¸ç•¶è¼•é¬†ã€‚
è‹¥å°‡ tile è¨­å¤ªå¤§ï¼Œå¯èƒ½å°è‡´ shared memory ä¸è¶³æˆ–æ˜¯ occupancy(åŒæ™‚åŸ·è¡Œçš„ block æ•¸é‡) é™ä½ã€‚è‹¥è¨­å¤ªå°ï¼Œå‰‡ kernel çš„å‘¼å«æ•¸é‡åˆæœƒè®Šå¤šï¼Œæ•ˆç‡ä¹Ÿæœƒä¸‹é™ã€‚
### Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.
1. QKDotAndScalarKernel<<<grid, block>>>
    - dim3 block(TILE_SIZE, TILE_SIZE) = (16, 16)ã€‚æ¯å€‹ block å…±æœ‰ 256 threads
    - dim3 grid((br+TILE_SIZE-1)/TILE_SIZE, (bc+TILE_SIZE-1)/TILE_SIZE) = (âŒˆ32/16âŒ‰,âŒˆ32/16âŒ‰)=(2,2)ï¼Œå› ç‚ºä¸€æ¬¡è¦è¦†è“‹ 32 x 32 çš„ S å€å¡Šï¼Œæ•…éœ€è¦ 2 x 2 = 4 å€‹ block ä¾†ä¸¦è¡Œè™•ç†ã€‚
    - Shared memory: éœ€è¦çš„ç©ºé–“ä¸»è¦æ˜¯ sQ[16][16] èˆ‡ sK[16][16]ï¼Œå…± 512 å€‹ floatï¼Œå¤–åŠ ä¸€é»é¡å¤–æš«å­˜ (å¹¾ä¹å¯ä»¥å¿½ç•¥ä¸è¨ˆ)ã€‚
2. RowMaxKernel<<<br,1>>>
    - ä¸€å€‹ block è² è²¬ä¸€å€‹ rowã€‚é€™è£¡ br=32 => gridDim=32ï¼ŒblockDim=1ã€‚
æ¯å€‹ row æœƒåœ¨è©² block çš„å–®ä¸€ thread è£é¢ç›´æ¥åšä¸€å€‹ for-loop æ‰¾æœ€å¤§å€¼ 
3. MinusMaxAndExpKernel<<<grid, block>>>
    - dim3 block(8,8)ï¼Œdim3 grid((br+7)/8, (bc+7)/8)ã€‚é€™å€‹ kernel éœ€è¦åŒæ™‚éæ­·æ•´å€‹ br x bc çŸ©é™£ï¼Œå› æ­¤ç”¨ 2D block (8,8)ã€‚æ¯å€‹ block æœ‰ 64 threadsï¼Œ(br+7)/8=4 => grid å¯èƒ½æ˜¯ (4,4) = 16 blocksï¼›16 blocks x 64 threads = 1024 threadsã€‚
4. RowSumKernel<<<br,1>>>
    -  èˆ‡ RowMaxKernel ä¸€æ¨£åšæ³•ï¼šä¸€å€‹ block è² è²¬ä¸€æ•´ row çš„åŠ ç¸½ã€‚
5. UpdateMiLiOiKernel<<<grid, block>>>
    -  dim3 block(32)ã€dim3 grid((br+31)/32) = 1 (å› ç‚º br=32)ã€‚
å³ 32 threads è² è²¬ 32 å€‹ row çš„æ›´æ–°ï¼ŒåŒæ™‚åœ¨ kernel è£é¢å†ç”¨ä¸€å€‹ for è¿´åœˆæŠŠ pij * V ç´¯ç©åˆ° Oã€‚

### Justify your choices and how they relate to the blocking factors and the SRAM size.
- Tiling å¤§å°è¨­ç‚º 16ï¼Œå¯ä¿è­‰åœ¨å¤§å¤šæ•¸ GPU ä¸Šèƒ½åŒæ™‚å®¹ç´å¤šå€‹ block åŸ·è¡Œï¼Œè€Œä¸æœƒéåº¦ä½”ç”¨ shared memoryã€‚16 x 16 x 4 bytes x 2ï¼ˆQ èˆ‡ Kï¼‰ â‰ˆ 2 KBï¼Œå°æ”¯æ´ 48KB~64KB shared memory çš„ SM ä¾†èªªè² æ“”å¾ˆå°ã€‚é€™æ¨£å°±èƒ½å¢åŠ  occupancyï¼ˆåŒæ™‚åŸ·è¡Œçš„ block/warp æ•¸é‡ï¼‰ï¼Œæå‡æ•´é«”ååé‡ã€‚
- ğ‘ğ‘Ÿ=32,ğ‘ğ‘=32 RowMaxKernel, RowSumKernel é€™é¡å° br=32 è¡Œåšæ“ä½œçš„ kernelï¼Œå¯ä»¥è¼•é¬†åœ°ç”¨ 32 threads (æˆ– 32 blocks with 1 thread) ä¾†å¹³è¡ŒåŒ–ã€‚32 è·Ÿ warp size åŒæ­¥ï¼Œè‹¥è¦æ”¹æˆä¸€è¡Œå°æ‡‰ä¸€å€‹ warpï¼Œä¹Ÿå®¹æ˜“åšå°é½Šï¼Œä¸æœƒæœ‰ thread é–’ç½®çš„å•é¡Œã€‚è‹¥æŠŠ br, bc è¨­å¾—å¤ªå¤§ï¼Œå¦‚ 64, 128ï¼Œå‰‡å–®æ¬¡éœ€è¦ç”¨åˆ°çš„ shared memory å°±æœƒè®Šå¤šï¼Œç”šè‡³è¦æ­é…æ›´è¤‡é›œçš„ tiling è¨­è¨ˆã€‚
åŒæ™‚ block æ•¸é‡æ¸›å°‘ï¼Œä¹Ÿæœ‰å¯èƒ½é™ä½ GPU æ ¸å¿ƒçš„ä½¿ç”¨ç‡ã€‚
- ä»¥ tile (16) çš„å¯¬åº¦åšè®€å¯«ï¼Œå¯ä»¥ä¿è­‰åŒä¸€ row æˆ–åŒä¸€åˆ—ä¸Šçš„é€£çºŒ thread æœƒè®€å–ç›¸é„°è¨˜æ†¶é«”ä½ç½®ï¼Œæ¸›å°‘å° global memory çš„ç„¡æ•ˆç‡å­˜å–ã€‚

- ç¤ºæ„åœ–
```SHELL=
æ•´é«” NxN (èˆ‰ä¾‹ N=8, br=4, bc=4)

   iBlock=0           iBlock=1
+---------+---------+
|  (0,0)  |  (0,1)  |   <- jBlock=0,1
+---------+---------+
|  (1,0)  |  (1,1)  |
+---------+---------+

æ¯ä¸€å¡Š(4x4)å…§éƒ¨ (QKDot) åˆç”¨ tile(16ç¶­) åœ¨ dç¶­åº¦ä¸Šåšéƒ¨åˆ† dot product
```
# Implementation (seq-attention.c )
[hw4_seq.cu](https://github.com/shirou1217/PP24/blob/main/hw4/hw4_seq.cu)
## kernelé…ç½®
- QKDotAndScalarKernel<<<blocksPerGrid, threadsPerBlock>>>
    - threadsPerBlock = 1024ï¼ŒblockDim.x = 1024ã€‚
    - æ¯å€‹ block å…§æœ‰ 32 å€‹ warp (WARPS_PER_BLOCK = 32)ã€‚
    blocksPerGrid = (N * N + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCKï¼Œ ä½¿å¾—æ¯å€‹ warp è² è²¬ä¸€å€‹ (i,j) çš„ä½ç½®ã€‚
    - Shared memory `__shared__ typename WarpReduce::TempStorage warp_temp_storage[32]`
- SoftMaxKernel<<<N, 256>>>
    - Grid ç¶­åº¦ï¼šgridDim.x = Nï¼Œè¡¨ç¤ºæ¯å€‹ block è² è²¬ä¸€å€‹ rowã€‚
    - Block ç¶­åº¦ï¼šblockDim.x = 256ï¼Œå³æ¯å€‹ row ç”¨ 256 å€‹ thread åšä¸¦è¡Œã€‚
    - Shared memory: `__shared__ float sdata[256];
__shared__ float row_max, row_sum;` 
- MulAttVKernel<<<blocksPerGridMV, threadsPerBlockMV>>>
    - threadsPerBlockMV = 256ã€‚
    - blocksPerGridMV = (N * d + threadsPerBlockMV - 1) / threadsPerBlockMVã€‚
    - æ¯å€‹ thread è² è²¬è¨ˆç®—ä¸€å€‹ (i,j) è¼¸å‡ºï¼Œå°æ‡‰è¦å°ä¸€æ•´ row (i) çš„ attention row ä¹˜ä»¥ V åœ¨ç¬¬ j ç¶­çš„å‘é‡åšå…§ç©ã€‚

## warp_reduce è®“æ•´é«”æ•ˆèƒ½å¤§å¹…æå‡çš„é—œéµ

- SoftMaxKernel
    - æ¯å€‹threadéƒ½æœƒåœ¨è‡ªå·±åˆ†é…åˆ°çš„å…ƒç´ ä¸­è¨ˆç®—å¾—å‡ºå±€éƒ¨æœ€å¤§local_maxä»¥åŠç¸½å’Œlocal_sumï¼Œæ¥è‘—é€éshare memory sdata[]é€²è¡Œtree-reductionï¼Œåœ¨é€™å€‹éç¨‹ä¸­ï¼Œæœƒæ¯”è¼ƒæ‰€æœ‰threadåœ¨è‡ªå·±å€åŸŸè¨ˆç®—å‡ºä¾†çš„local_maxå¾ä¸­æŒ‘é¸å‡ºæœ€å¤§çš„å­˜åœ¨sdata[0]ï¼Œä»¥åŠå°‡æ‰€æœ‰local_sumåŠ èµ·ä¾†æˆç‚ºrow_sumå­˜åœ¨sdata[0]ï¼Œæœ€å¾Œå†å°‡d_attå…ƒç´ /row_sum å®Œæˆè¨ˆç®—
    - Warp-level reduce å°‡æ•´å€‹ warp çš„ Thread ä½œç‚ºä¸€å€‹å–®ä½é€²è¡ŒåŒæ­¥å’Œ reduce æ“ä½œï¼Œæ¸›å°‘äº†è·¨ Thread Block çš„åŒæ­¥éœ€æ±‚ï¼Œé™ä½äº†å…±äº«è¨˜æ†¶é«”çš„ç«¶çˆ­ã€‚
    - åˆ©ç”¨ warp çš„å…§éƒ¨åŒæ­¥ç‰¹æ€§ï¼ˆå³åŒä¸€ warp çš„ Thread æ˜¯åŒæ­¥åŸ·è¡Œçš„ï¼‰ï¼Œé¿å…äº†å…¨å±€åŒæ­¥çš„å»¶é²ï¼Œæå‡äº† reduce æ“ä½œçš„æ•ˆç‡ã€‚
- QKDotAndScalarKernel
- æ¯å€‹warpä¸­çš„æ¯å€‹threadè² è²¬åˆ†æ“”ä¸€éƒ¨ä»½çš„dot productå› æ­¤æ¯å€‹warpæœƒæœ‰è‡ªå·±è¨ˆç®—çš„å±€éƒ¨sumï¼Œé€éWarpReduceå°‡æ‰€æœ‰warpçš„è¨ˆç®—å‡ºä¾†çš„å±€éƒ¨sumåŠ ç¸½åœ¨ä¸€èµ·ï¼Œä¸¦ç”±thread 0 å°‡å€¼å¯«å›global memoryä¸­ã€‚


# Profiling Results
![è¢å¹•æ“·å–ç•«é¢ 2024-12-25 180707](https://hackmd.io/_uploads/S1imzDKBJg.png)
ç”±åœ–ä¸­å¯ä»¥ç™¼ç¾ç”±æ–¼SoftMaxKernel,QKDotAndScalarKernelä¸­æˆ‘æœ‰å¯¦ä½œwarp_reduceä»¥åŠtree-reductionå› æ­¤é€™ä½¿å¾—ä»–å€‘è³‡æºåˆ©ç”¨ç‡å‡æ¯”å…¶ä»–kernelä¾†çš„é«˜ï¼Œachieved_occupancyå’Œsm_efficiencyå‡é«˜ï¼Œæ­¤å¤–SoftMaxKernelæˆ‘æœ‰ä½¿ç”¨share memoryå› æ­¤å¯ä»¥ç™¼ç¾ä»–çš„shared memory load/store throughputå‡æ¯”å…¶ä»–kernelä¾†çš„é«˜ã€‚
# Experiment & Analysis
![testcase=t15](https://hackmd.io/_uploads/HkWAYUFSkl.png)
ç¨‹å¼ä½¿ç”¨share memoryå’Œwarp_reduceèƒ½æœ‰æ¯”è¼ƒå¤§çš„æ•ˆèƒ½å„ªåŒ–

# Experience & conclusion
- What have you learned from this homework?
- äº†è§£åˆ°SRAMå°æ–¼æ•ˆèƒ½æå‡çš„é‡è¦æ€§ï¼ŒæŒæ¡äº†å°‡å¤§å‹çŸ©é™£åŠƒåˆ†ç‚ºå°å€å¡Šï¼ˆtilesï¼‰é€²è¡Œè¨ˆç®—çš„æ–¹æ³•ï¼Œå­¸æœƒwarp-level reduce æŠ€è¡“

